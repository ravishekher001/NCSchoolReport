{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1  MSDS Data Mining -- North Carolina Education Data Set\n",
    "\n",
    "** Austin Hancock, Shravan Kuchkula, Kevin Okiah, Damarcus Thomas **\n",
    "\n",
    "![](http://dthomas.mathematical.guru/Img/NCZones.png)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "> The North Carolina Department of Public Instruction (NCDPI) is charged with implementing the state's public school laws and the State Board of Education's policies and procedures governing pre-kindergarten through 12th grade public education. The elected State Superintendent of Public Instruction leads the Department and functions under the policy direction of the State Board of Education.\n",
    "\n",
    "> The agency provides leadership and service to the 115 local public school districts and 2,500+ traditional public schools, 150+ charter schools, and the three residential schools for students with hearing and visual impairments. The areas of support include curriculum and instruction, accountability, finance, teacher and administrator preparation and licensing, professional development and school business support and operations.\n",
    "\n",
    "> The NCDPI develops the Standard Course of Study, which describes the subjects and course content that is taught in North Carolina public schools, and the assessments and accountability model used to evaluate student, school and district success.\n",
    "\n",
    "> The NCDPI administers annual state and federal public school funds totaling approximately $9.2 billion and licenses the approximately 117,000 teachers and administrators who serve public schools. The NCDPI's primary offices are in Raleigh, with four regional alternative licensing centers in Concord, Fayetteville, Elm City and Catawba. Approximately 30,000 new teacher and administrator licenses are issued annually from these centers. The NCDPI's work extends to the NC Center for the Advancement of Teaching with locations in Cullowhee and Ocracoke, and the NC Virtual Public School – the second largest virtual public school in the nation. The state agency also works closely with nine Regional Education Service Alliances/ Consortia and six regional accountability offices.\n",
    "\n",
    "** The mission, vision and goals of the Public Schools of North Carolina is as follows: **\n",
    "\n",
    "**VISION:**\n",
    "> Every public school student will graduate ready for post secondary education and work, prepared to be a globally engaged and \n",
    "> productive citizen\n",
    "\n",
    "\n",
    "**MISSION:**\n",
    "> The State Board of Education has the constitutional authority to lead and uphold the system of public education in North \n",
    "> Carolina.\n",
    "\n",
    "\n",
    "**GOALS:**\n",
    "> Goal: Every student in the NC Public School System graduates from high school prepared for work, further education and \n",
    "> citizenship.\n",
    ">\n",
    "> Goal: Every student has a personalized education.\n",
    ">\n",
    "> Goal: Every student, every day has excellent educators.\n",
    ">\n",
    "> Goal: Every school district has up-to-date financial, business, and technology systems to serve its students, parents and \n",
    "> educators.\n",
    ">\n",
    "> Goal: Every student is healthy, safe, and responsible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* __Describe the purpose of the data set you selected (i.e., why was this data collected in the first place?)__\n",
    "      \n",
    "> For this project will be using North Carolina High Schools education data. The data is a combination of multiple data tables publicly available by the North Carolina Department of Public Instruction (NCDPI)  which is charged with implementing the state's public school laws and the State Board of Education's policies and procedures governing pre-kindergarten through 12th grade public education. You can learn more about the data by visiting http://www.ncpublicschools.org/data/reports/\n",
    "\n",
    "> The data is collected in an effort to compile all measurable state-wide statistics on schools in North Carolina to inform and \n",
    "educate all stakeholders on the state of the educational system.  \n",
    "\n",
    "> Once compiled and preprocessed, the data could then be mined to answer multiple questions like: what are key factors that \n",
    "increase SAT/ACT scores, and how can schools retain masterful teachers, and ultimately, identify factors within schools that \n",
    "can increase graduation rates. \n",
    "\n",
    "> The data should be harnessed to hightlight ways and strategies on how to improve the strengths and weaknesses of the school system at large.\n",
    "     \n",
    "    \n",
    "* __Describe how you would define and measure the outcomes from the dataset.__  \n",
    "\n",
    "> Our initial goal is to understand how the education system in North Carolina is at the moment and interpretating the State  theme 'Educating the Whole Child'. \n",
    "\n",
    "> Examining the mission and vision of the public school systems would help guide us to zoom in on certain objective measureable criteria within the data, like how can the such as increase trends in various school features that examine both formative and summative scores and quantitative and  qualativity data that are indicitive of the essentials of a successful school system. \n",
    "\n",
    "> What parts of the state  have low test scores? Do some schools have better access to computers for the students? How does the state allocate funds to the districts? By investigating these questions, and others like them, we will have a more wholistic understanding of the dataset. \n",
    "          \n",
    "* __How would you measure the effectiveness of a good prediction algorithm?__ \n",
    "\n",
    "> Our team aims to predict the graduation rate of High Schools in the state of North Carolina based on the multiple features taken from the North Carolina dataset motivated by the mission vision and goals of the education system. To access the effectiveness of our algorithm we will begin by performing regression diagnostics for any unusual data and nonlinearity in our model. We will look at MSE, MAE, or RMSE among others to see how well our model fits the data. To guage the accuracy of our model, a 5 fold Cross-Validation will be performed.\n",
    "\n",
    "> If the results shows innovation and suggestions what is known best practices or even better leads to new information that was unseen and forecasted by tradition means and current policies, procedures, and protocals.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 478 entries, 0 to 477\n",
      "Columns: 294 entries, Unnamed: 0 to Percent GLP\n",
      "dtypes: bool(8), float64(264), int64(9), object(13)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# The normal imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# Import the stats library\n",
    "from scipy import stats\n",
    "\n",
    "# These are the plottinglibraries we'll use:\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "plt.style.use(\"classic\")\n",
    "import seaborn as sns\n",
    "\n",
    "#Machine learning\n",
    "\n",
    "\n",
    "# Command for plots to appear in the iPython Notebook\n",
    "%matplotlib inline\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Starting with the dataset that Dr.Drew helped clean.->highschools saved to .cvs file from graduations rates (2) notebook\n",
    "wd = os.getcwd() #get working directory\n",
    "highschools = pd.read_csv(wd+'/data/highschools.csv', low_memory=False)\n",
    "#highschools = pd.read_csv(wd+'\\\\data\\\\highschools.csv', low_memory=False)\n",
    "highschools.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Meaning Type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/Shravan/R/projects/NCSchoolReport/data/DataMeaningType.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-16d3a655c3e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_rows'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_colwidth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/data/DataMeaningType.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Shravan/anaconda/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheetname, header, skiprows, skip_footer, index_col, names, parse_cols, parse_dates, date_parser, na_values, thousands, convert_float, has_index_names, converters, true_values, false_values, engine, squeeze, **kwds)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     return io._parse_excel(\n",
      "\u001b[0;32m/Users/Shravan/anaconda/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, **kwds)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[0;32m/Users/Shravan/anaconda/lib/python3.6/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/Shravan/R/projects/NCSchoolReport/data/DataMeaningType.xlsx'"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500) \n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "pd.read_excel(wd+'/data/DataMeaningType.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Dictionary\n",
    "> Since this datasets has numerous columns we needed a fast way to quickly find ColumnName description for easy reference. For exceptional work, we created a function to quickly pull the data from csv datafile. This involved converting the pdf to excel and formating the data for easy import into pandas. The code below is a working code for our data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a simple function to pull column description\n",
    "DataDict = pd.read_excel(wd+'\\\\data\\\\DataMeaningType.xlsx', encoding = \"ISO-8859-1\")\n",
    "DataDict.head()\n",
    "#DataDict = DataDict.columns['COLUMN_NAME', 'DESCRIPTION']\n",
    "def get_ColDescription(colname = 'Year'):\n",
    "    colName = input(\"Enter column name to check description in Dictionary. You can enter multiple columns separated by comma: \")\n",
    "    \n",
    "    print('You entered: ', colName.strip())\n",
    "    temp = pd.DataFrame()\n",
    "    colNames = colName.split(',')\n",
    "    \n",
    "    try:\n",
    "        for i in range(0,len(colNames)):\n",
    "            get = (DataDict[DataDict.Attribute==colNames[i].strip().lower()])\n",
    "            temp = temp.append(get)\n",
    "        return(temp)\n",
    "    except Exception as e:\n",
    "        print(e.args) \n",
    "\n",
    "get_ColDescription()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get dimensions of the dataframe that we working with\n",
    "highschools.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We begin by checking the dimensions of our dataset. We have 478 rows and 294 columns to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make a copy of the dataset to work with\n",
    "HighschoolData = highschools.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Missing Data Statistics*\n",
    "\n",
    "> In this section we explore missing data in the highschool data that was created from the original north carolina data. After exploring the missing data we will discusss the approach for handling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check for missing values\n",
    "Temp = pd.DataFrame(HighschoolData.isnull().sum())\n",
    "#print(Temp)\n",
    "Temp.columns =['DataMissing']\n",
    "#Columns with atleast 1 missing value\n",
    "MissingCount = Temp[Temp.DataMissing>0]\n",
    "\n",
    "#sort \n",
    "MissingCount = MissingCount.sort_values('DataMissing',ascending=False)\n",
    "#basic starts on missing data\n",
    "print(\"##################################################\")\n",
    "print(\"###\", '             Stats on missing data        ',  \"###\")\n",
    "print(\"##################################################\")\n",
    "print('No of rows in HighschoolData: ',len(HighschoolData))\n",
    "print('No of Columns in HighschoolData: ',len(HighschoolData.columns))\n",
    "print('No. of Columns with Missing data: ',len(MissingCount))\n",
    "print('No  of Complete data columns: ', len(HighschoolData.columns) - len(MissingCount))\n",
    "print('No. of Columns with 100% missing Values: ',sum(1 for item in MissingCount.DataMissing if item==len(HighschoolData)))\n",
    "print('No. of Columns more than 95% missing Values: ',sum(1 for item in MissingCount.DataMissing if item>=0.95*len(HighschoolData)))\n",
    "print('% of Columns with some Missing data: ',round(float(100*len(MissingCount))/len(HighschoolData.columns),3),'%')\n",
    "print('% of Columns with 100% Missing data: ',round(float(100*sum(1 for item in MissingCount.DataMissing if item==len(HighschoolData)))/len(HighschoolData.columns),3),'%')\n",
    "print('% of Columns with more than 95% Missing data: ',round(float(100*sum(1 for item in MissingCount.DataMissing if item>=0.95*len(HighschoolData)))/len(HighschoolData.columns),3),'%')\n",
    "\n",
    "print(\" \")\n",
    "print(\"##################################################\")\n",
    "print(\"###\",'       Columns with >95% missing data     ',    \"###\")\n",
    "print(\"##################################################\")\n",
    "#add a columnName for Bar charts plot\n",
    "MissingCount['ColumnName'] = MissingCount.index\n",
    "MissingCount['Missing %'] = 100*(MissingCount.DataMissing/len(HighschoolData))\n",
    "MissingCount = MissingCount.reset_index()\n",
    "del MissingCount['index']\n",
    "print((MissingCount.head(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From above we note, of the 294 columns in HighSchoolData, 168 (57.14%) of them have some missing data. 47 (15.99%) of columns have all the data missing where 99 (33.67%) of the columns miss more than 95% of the data. For columns with 100% missing data we are left but to wonder if this were new fields introduced recently and the schools are not fully informed to collect this data or the data for this columns was not just collected by mistake. On the other hand, for the partally completed datasets, some schools might have left these columns blank if they didn't apply to them.\n",
    "\n",
    "> For the sake of this analysis we will drop the columns that have `>95` missing data rather than try complete the missing data with either mean, median or mode. If we complete this large amount of missing rows of data with predetermined data, our model maybe baised and throw were predictions off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#All columns from Original schoolData\n",
    "AllColumns =HighschoolData.columns \n",
    "\n",
    "# All columns with some missing value\n",
    "ColumnsWithMissingdata = MissingCount.ColumnName \n",
    "\n",
    "#columns with more than 95% missing data\n",
    "Columns2Drop = MissingCount.ColumnName.head(99) \n",
    "\n",
    "#columns with missing values that are kept\n",
    "Columns2Keep = MissingCount.ColumnName.tail(len(MissingCount)-99) \n",
    "\n",
    "#All the other columns except those with >95% missing data\n",
    "SelectedColumns = list(set(AllColumns)-set(Columns2Drop)) \n",
    "\n",
    "#new dataset, columns with >95% missing data dropped\n",
    "\n",
    "schoolDataNew = HighschoolData[SelectedColumns]\n",
    "\n",
    "#drop the key column. \n",
    "del schoolDataNew['unit_code']\n",
    "\n",
    "print(\"##################################################\")\n",
    "print(\"###\", '          Orginal Highschooldata info     ',  \"###\")\n",
    "print(\"##################################################\")\n",
    "print(HighschoolData.info())\n",
    "print(\"##################################################\")\n",
    "print(\"###\", '  Columns with >95 data missing   dropped ',  \"###\")\n",
    "print(\"##################################################\")\n",
    "print(schoolDataNew.info())\n",
    "schoolDataNew.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the code block above, we drop 99 columns which have >95% of the data missing. We also delete the unit_code column which is a key to identify the school/Lea/state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Exploring the columns with missing dataset that are retained\n",
    "df = schoolDataNew[Columns2Keep]\n",
    "temp_df = df.copy() \n",
    "print(\"##################################################\")\n",
    "temp  = df.select_dtypes(include=['int','float'])\n",
    "temp2  = df.select_dtypes(include=['object', 'bool'])\n",
    "print(\"#      Columns with continious data (int, float) #\")\n",
    "print(\"##################################################\")\n",
    "temp.info()\n",
    "print(\" \")\n",
    "print(\"##########################################\")\n",
    "print(\"#     Columns with Categorical data      #\")\n",
    "print(\"##########################################\")\n",
    "temp2.info()\n",
    "print(\" \")\n",
    "print(\"Total # of columns: \",len(Columns2Keep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the code block above we explore the columns with missing data that we retained. we group the columns into two based on data type as we will have two different approaches for replacing the missing data;\n",
    "1. #### Columns with continuous data types (ints and floats)\n",
    "Of the 69 columns retained with missing data, 64 are of dtype float64. After reviewing individual columns description from the data dictionary, we choose to replace these missing float types with median. We didn't go with mean simply because with existance of outliers, mean would be screwed compared to median.\n",
    "2. #### Columns with contegorical data types (objects and bool)\n",
    "For the 5 categorical columns, we will be replacing the missing values with mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#this function replaces NA's for columns with continous 'Con' (int or float) variables with median and categorical 'Cat' variable(bool or object) with mode\n",
    "def ReplaceMissingdata(df = schoolDataNew):\n",
    "    temp_df = df.copy() \n",
    "    print(\"#######################################\")\n",
    "    print(\"## continous and categorical columns ##\")\n",
    "    print(\"#######################################\")\n",
    "    temp  = df.select_dtypes(include=['int64','float'])    #continuous values columns\n",
    "    columnnames = temp.columns\n",
    "    temp1  = df.select_dtypes(include=['object','bool']) #catagorical values columns\n",
    "    columnnames1 = temp1.columns\n",
    "    print(\" \")\n",
    "    \n",
    "    print(\"##############################\")\n",
    "    print(\"## NA count Before Cleaning ##\")\n",
    "    print(\"##############################\")\n",
    "    print(df.isnull().sum())\n",
    "    #replacing missing continous values with median\n",
    "    for i in range(0,len(columnnames)):\n",
    "        try:\n",
    "            temp_array =temp[temp[columnnames[i]]!=np.nan][columnnames[i]] #temp array of non NAs for continous values to calculate median\n",
    "            # replace NAs with median for continous variables created from above arrays\n",
    "            temp_df[columnnames[i]] =temp_df[columnnames[i]].replace(np.nan,temp_array.median())     \n",
    "        except Exception as e:\n",
    "            print(e.args) \n",
    "            \n",
    "    #replacing missing contegorical values with mode\n",
    "    for i in range(0,len(columnnames1)):\n",
    "        try:\n",
    "            temp_array1 =temp1[temp1[columnnames1[i]]!=np.nan][columnnames1[i]] #temp array of non NAs for categorical values to calculate mode\n",
    "            # replace NAs with median for categorical values created from above arrays with mode\n",
    "            temp_df[columnnames1[i]] =temp_df[columnnames1[i]].replace(np.nan,str(temp_array1.mode()[0]))      \n",
    "        except Exception as e:\n",
    "            print(e.args)\n",
    "\n",
    "    print(\"##############################\")\n",
    "    print(\"## NA Count After Cleaning  ##\")\n",
    "    print(\"##############################\")\n",
    "    print(temp_df.isnull().sum())\n",
    "    df = temp_df\n",
    "    return df\n",
    "\n",
    "new_schooldata = ReplaceMissingdata(schoolDataNew)    \n",
    "\n",
    "new_schooldata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the code block above, we replaced missing catagorical values with mode and missing continuous values with median. A total of 69 columns with missing values have been cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_schooldata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Our final dataset has 478 rows and 194 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function for getting column description from the data dictionary. \n",
    "# It is at the bottom of the notebook in the exceptional work section. Run it first before calling it in this cell\n",
    "#get_ColDescription()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We will be using the `new_schooldata` dataframe going forward for calculating the simple statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a copy of the dataset to work with\n",
    "statsSchoolData = new_schooldata.copy()\n",
    "\n",
    "# Extract Bool, Object and Numeric types into seperate df's\n",
    "statsSchoolDataBool = statsSchoolData.loc[:, statsSchoolData.dtypes == bool]\n",
    "statsSchoolDataObject = statsSchoolData.loc[:, statsSchoolData.dtypes == object]\n",
    "statsSchoolDataNumeric = statsSchoolData.loc[:, (statsSchoolData.dtypes == float) | (statsSchoolData.dtypes == 'int64')]\n",
    "\n",
    "# Remove spaces from column names. Eg: \"State Gap Compared\" becomes \"State_Gap_Compared\"\n",
    "statsSchoolDataBool.columns = statsSchoolDataBool.columns.str.replace('\\s+', '_')\n",
    "statsSchoolDataObject.columns = statsSchoolDataObject.columns.str.replace('\\s+', '_')\n",
    "statsSchoolDataNumeric.columns = statsSchoolDataNumeric.columns.str.replace('\\s+', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DatatypesFreq =  pd.DataFrame({'Datatype':['Boolean','Object','Numeric'],'Freq':[len(statsSchoolDataBool.columns),len(statsSchoolDataObject.columns)\n",
    "                                ,len(statsSchoolDataNumeric.columns)]})\n",
    "\n",
    "DatatypesFreq.Freq.groupby(DatatypesFreq.Datatype).sum().plot(kind='pie',autopct='%.2f%%', figsize=(6, 6))\n",
    "my_colors = ['b', 'r', 'c']\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Majority of the attributes in this dataset is Numeric (float64 and int64) which accounts for 90.7% of the data.  Boolean and Object datatypes account for 4.12% and 5.15% of the data respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boolean Variables Simple Stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boolDescribe = statsSchoolDataBool.describe().T\n",
    "def proportions(mySeries):\n",
    "    return mySeries.value_counts(normalize=True).values[0]\n",
    "boolDescribe['percent_false'] = 100 * statsSchoolDataBool.apply(proportions)\n",
    "boolDescribe['percent_true'] = 100 * (1 - statsSchoolDataBool.apply(proportions))\n",
    "boolDescribe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Some interesting observations, 37% of schools have Grad_project_status and summer_program_flg, it will be interesting to see if they are the same set of school profiles and do this kind of programs actually improve the overall student success in terms of test score and graduation rate. Similarly, 11% of schools have clp_ind_flg and focus_clp_flg which could mean that schools which have clp_ind_flg might also have focus_clp_flg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Object Variables Simple Stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statsSchoolDataObject.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From above, we learn that most of the high schools in NC follow the Traditional calendar with an exception of 8 who follow the all year round calendar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numeric Variables Simple Stats:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An outlier is a data point that is distinctly separate from the rest of the data. One definition of outlier is any data point more than 1.5 interquartile ranges (IQRs) below the first quartile or above the third quartile. First we begin by calculating if a column contains atleast 1 outlier by comparing the max and min values with the outlier range. Next, we determine how many data points in a column are considered as outliers. We report this statistics below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.reset_option('display.max_rows')\n",
    "numericDescribe = (statsSchoolDataNumeric.describe(include='all').T).round(decimals=3)\n",
    "\n",
    "# Calculate outliers using this formula: first quartile – 1.5·IQR > outlier > third quartile + 1.5·IQR\n",
    "numericDescribe['IQR'] = numericDescribe['75%'] - numericDescribe['25%']\n",
    "numericDescribe['outliers'] = (numericDescribe['max'] > (numericDescribe['75%'] + (1.5 * numericDescribe['IQR']))) \\\n",
    "                            | (numericDescribe['min'] < (numericDescribe['25%'] - (1.5 * numericDescribe['IQR'])))\n",
    "\n",
    "# Calculate IQR for each column of the dataframe.\n",
    "IQR = statsSchoolDataNumeric.quantile(.75) - statsSchoolDataNumeric.quantile(.25)\n",
    "\n",
    "# Calculate the upper and lower outlier values\n",
    "upperOutlier = statsSchoolDataNumeric.quantile(.75) + (1.5 * (IQR))\n",
    "lowerOutlier = statsSchoolDataNumeric.quantile(.25) - (1.5 * (IQR))\n",
    "\n",
    "# Check if each value of mySeries lies within the outlier range.\n",
    "# If the value is greater than upperOutlier mark it True\n",
    "# If the value is lower than lowerOutlier mark it True\n",
    "# return the total number of outliers for that series.\n",
    "def numberOfOutliers(mySeries):\n",
    "    return sum((mySeries > upperOutlier.loc[mySeries.name, ]) |\\\n",
    "               (mySeries < lowerOutlier.loc[mySeries.name,]))\n",
    "\n",
    "# Store the result in a new column\n",
    "numericDescribe['num_outliers'] = statsSchoolDataNumeric.apply(numberOfOutliers)\n",
    "numericDescribe.sort_values('num_outliers', ascending=False, inplace=True)\n",
    "newColOrder = ['count', 'outliers', 'num_outliers', 'IQR', 'mean', \\\n",
    "               'min', '25%', '50%', '75%', 'max', 'std']\n",
    "numericDescribe.reindex(columns=newColOrder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numericDescribe.outliers.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Select 5 columns with most outliers to display violin plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top5df = statsSchoolDataNumeric[numericDescribe.index.values[0:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(top5df.columns)):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    sns.violinplot(x = top5df.iloc[:,i], data=top5df, inner=None, color='lightgray')\n",
    "    sns.stripplot(x = top5df.iloc[:,i], data=statsSchoolDataNumeric, size=4, jitter=True)\n",
    "    #plt.title(top5df.iloc[:,i].name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The effect of outliers can be seen. Some are heavily left-skewed and some are heavily right-skewed. Implications of having outliers must be carefully studied. For now we choose to retain all the outlier and we will revisit them during modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = sns.lmplot(x = 'Avg_Class_Size_MathI', y = 'Math_I_Score', data=statsSchoolDataNumeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# School Category Factor plot \n",
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "sns.countplot(\"category_cd\",data=new_schooldata)\n",
    "new_schooldata.groupby([\"category_cd\"]).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Of the 478 High schools, 459 are of fall in  category H ,15 of catagory T and 4 of category A. See summary table below for school category descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category | Description|Count |Percentage\n",
    "--- | --- | ---|---\n",
    "H | Schools with high school grades 9-13 | 459|96.02%\n",
    "T | schools with middle and high school grades | 15|3.14\n",
    "A | Schools with elementary, middle and high school grades | 4|0.84%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#histograms of teacher experience\n",
    "#with sns.axes_style(\"darkgrid\"):\n",
    "X =new_schooldata.Tch_Exp_Pct_0_3_Years\n",
    "Y  =new_schooldata.Tch_Exp_Pct_4_10_Years\n",
    "Z =new_schooldata[\"Tch_Exp_Pct_10+_Years\"]\n",
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "bins = 70\n",
    "figure_title = \"Histogram of % of Teachers at a given experience Level at High Schools in NC\"\n",
    "plt.title(figure_title, y=1.08)\n",
    "plt.hist(X, bins, alpha=0.7, label='0-3 Years')\n",
    "plt.hist(Y, bins, alpha=0.7, label='4-10 Years')\n",
    "plt.hist(Z, bins, alpha=0.7, label='+10 Years')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From Histograms above most schools have teachers with ~20% experience between 4-10 Years. There is little difference in distributions of percentages of teachers with 0-3 years and 4 to 10 years. There is one outlier school where 100% of the teachers have more that 10 years of experience. It will be interesting to if teacher experience actually translates to increased graduation rate or even better performance in assessment tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "A =new_schooldata.avg_daily_attend_pct\n",
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "figure_title = \"% Average daily attendance NC\"\n",
    "plt.title(figure_title, y=1.08)\n",
    "bins = 70\n",
    "plt.hist(A, bins, alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Most schools have pretty good average daily attendance percentage >90%. We have only one school struggling with average daily attendance of ~82%. Further "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Joint Attributes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. School Performance Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Categorical columns for joint plots explorations\n",
    "temp = new_schooldata.select_dtypes(['bool','object'])\n",
    "\n",
    "#store the selected columns list in array\n",
    "categorical_cols = temp.columns\n",
    "print('Categorical columns')\n",
    "print('-------------------------------')\n",
    "print(categorical_cols)\n",
    "\n",
    "print('-------------------------------')\n",
    "print('')\n",
    "print('-------------------------------')\n",
    "print('Educator Experience')\n",
    "#list of columns for teacher experience measures\n",
    "teacher = [col for col in new_schooldata.columns if 'Tch' in col]\n",
    "Teacher = [teacher[1], teacher[2],teacher[6]]\n",
    "print(Teacher)\n",
    "print('-------------------------------')\n",
    "print('')\n",
    "print('-------------------------------')\n",
    "#List of Columns for achivement score measures\n",
    "#Achivement_measures  = ['The ACT Score','Overall Achievement Score','sch_percent_college_enrolled_16_mos_post_grad','EVAAS Growth Score','Math Course Rigor Score','sat_avg_score_num','Cohort Graduation Rate Standard Score']\n",
    "Achievement_scores = [col for col in new_schooldata.columns if 'score' in col]\n",
    "Achievement_Scores = [col for col in new_schooldata.columns if 'Score' in col]\n",
    "\n",
    "#combine the above two arrays\n",
    "Achievement_measures = Achievement_Scores+Achievement_scores \n",
    "print('Achievement Measures columns')\n",
    "print('-------------------------------')\n",
    "Achievement_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# factor plots for scores vs teacher experience considering categorical factors\n",
    "for i in range(0, len(Achievement_measures)):\n",
    "    sns.factorplot('category_cd',Achievement_measures[i],hue='summer_program_flg',data=new_schooldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sort the x-axis (catagory_cd) from above\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    new_schooldata2=new_schooldata.sort_values(axis =0, by='category_cd')\n",
    "    for i in range(0, len(Achievement_measures)):\n",
    "        sns.factorplot('category_cd',Achievement_measures[i],data=new_schooldata2,hue='summer_program_flg', size = 6, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sns.set_style(\"dark\")\n",
    "\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    sns.factorplot('category_cd','The ACT Score',data=new_schooldata2,size=5, aspect=2)\n",
    "    plt.title('THE ACT SCORE by School Category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From the above three code blocks, a strong correlation exists between School catagory_cd and its performance in the different performance and test scores measures.Considering Math Score,The ACT Score, Biology Score, EVAAS Growth Score and cohort Graduation Rate Standard score, schools category  A (Schools with elementary, middle and high school grades) performed much better followed by category H (Schools with high school grades 9-13) then Schools T (schools with middle and high school grades). We show one plot for ACT which displays the trend being described. Since the number of Schools of category A=4  and T=15 are less than the 30 samples, though a posible trend exists, we do not draw a statistical conclusion on performance based on school catagory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#finding a list of the most correlated features\n",
    "c = new_schooldata.corr()#.abs()\n",
    "s =pd.DataFrame(c.unstack())\n",
    "#so = s.order(kind=\"quicksort\")\n",
    "s['Correlation Pair'] = s.index\n",
    "s = s.reset_index()\n",
    "#del s['index']\n",
    "s.columns = ['Column1', 'Column2', 'correlation','Correlation Pair']\n",
    "result = s.sort_values('correlation', ascending=0)\n",
    "result1 = result[result.correlation<-0.3]\n",
    "result2 = result[result.correlation>0.3]\n",
    "\n",
    "frames =[result2, result1]\n",
    "\n",
    "result3 = pd.concat(frames)\n",
    "result3[result3.Column1.isin(['Cohort Graduation Rate Standard Score'])]\n",
    "\n",
    "#result2[result2.Column1.isin(['Cohort Graduation Rate Standard Score'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From the correlation table above afew things are learned. As expected, there exists a strong positive correlation between standardized tests and cohort graduation rate. Though one could have expected Cohort graduation rate to be strongly correlated to Average daily attendance 'avg_daily_attend_pct', the correlation comes in as a moderate  0.44. There exists a moderate negative correlation (-0.48) between short term suspension (short_susp_per_c_num) and Cohort graduation rate. \n",
    "\n",
    "> Below we show joint regreation plot to illustrate positive correlation between Cohort graduation rate standard Score and Overrall achievement School and one negative correlation between Cohort Graduation Rate Standard Score and  short_susp_per_c_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"dark\")\n",
    "sns.jointplot(new_schooldata['Cohort Graduation Rate Standard Score'], new_schooldata['Overall Achievement Score'], size=7, kind ='reg')\n",
    "sns.jointplot(new_schooldata['Cohort Graduation Rate Standard Score'], new_schooldata['short_susp_per_c_num'], size=7, kind ='reg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Attributes and Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<div class='tableauPlaceholder' id='viz1516831600677' style='position: relative'><noscript><a href='#'><img alt='Dashboard 1 ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Cr&#47;CrimeRates_5&#47;Dashboard1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='CrimeRates_5&#47;Dashboard1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Cr&#47;CrimeRates_5&#47;Dashboard1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1516831600677');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Geographical maps showing crime and graduation data by zip code. (Graduation Rate color scale centered at 80%) (Hover to see see data by zip code. Adjust crime rate scale to filter both maps)\n",
    "\n",
    "> The Crime Rates map displays the number of crimes per 100 students for a given zip code (the rates are taken at a log scale due to the scale of the data). The Graduation Rates map shows the 4 year graduation rates for a given zip code. Our initial thought was that a geographic region with high crime rates would also have low graduation rates. When we filter off regions with lower crime rates, we can see that the areas that remain in the Graduation Rates map contain regions with low graduation rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1\n",
    "#################################################\n",
    "#               New Features                    #\n",
    "#################################################\n",
    "# create Factors for 'Overall Achievement Score'\n",
    "# A: 85-100\n",
    "# B: 70-84\n",
    "# C: 55-69\n",
    "# D: 40-54\n",
    "# F:<40\n",
    "new_schooldata = new_schooldata.assign(Overall_Achievement_Score_bins = pd.cut(np.array(new_schooldata['Overall Achievement Score']), \n",
    "                                              5, labels=[\"<40\", \"40-54\", \"55-69\",'70-84',\"85-100\"]))\n",
    "#2\n",
    "#SchoolWeightedScore:  Summation of all achievement measures Scores.\n",
    "bin1 = ['EVAAS Growth Score','Overall Achievement Score' ] # Weight 0.6\n",
    "bin2 = ['The ACT Score','ACT WorkKeys Score' ,'Math Course Rigor Score'] # weight 0.3\n",
    "bin3 = ['Math I Score', 'English II Score','Biology Score'] # EOC scores weight 0.1\n",
    "#print(Achievement_measures)\n",
    "new_schooldata = new_schooldata.assign(BIN1=new_schooldata[bin1].sum(axis=1)*.6)\n",
    "new_schooldata = new_schooldata.assign(BIN2=new_schooldata[bin2].sum(axis=1)*.3)\n",
    "new_schooldata = new_schooldata.assign(BIN3=new_schooldata[bin3].sum(axis=1)*.1)\n",
    "groupedbins = ['BIN1','BIN2','BIN3']\n",
    "new_schooldata = new_schooldata.assign(SchoolWeightedScore = new_schooldata[groupedbins].sum(axis =1))\n",
    "\n",
    "new_schooldata.T\n",
    "\n",
    "#3\n",
    "# income features/ social economic/ Per capita\n",
    "\n",
    "#4 \n",
    "#population data from state census data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From above code, we have created two new features from our dataset.\n",
    "\n",
    "> 1. Overall_Achievement_Score_bins:\n",
    "this is to groub the Overall Achievement Score into bins \n",
    "A: 85-100,\n",
    "B: 70-84,\n",
    "C: 55-69,\n",
    "D: 40-54,\n",
    "F:<40\n",
    "> 2. SchoolWeightedScore: Summation of all achievement measures Scores.`EVAAS Growth Score` and `Overall Achievement Score` are put on bin1 and given a  weight 0.6 of the overall weighted school school.`The ACT Score`, `ACT WorkKeys Score` , and `Math Course Rigor Score` are put in bin 2 carrying a weight of 0.3 while bin3 contains End of Course scores for `Math I Score`, `English II Score`, and `Biology Score` given a weight 0.1. When we get to lab two, we will use this features to see if they improve our model performance.\n",
    "\n",
    "Refer to visualizations below for this two newly created features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.countplot(x=\"Overall_Achievement_Score_bins\", data=new_schooldata )\n",
    "plt.title('Overall_Achievement_Score Binning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Binning distribution of Overall achievement score is normal. Majority of the High schools in NC fall in the C grade bin (55-69). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot(new_schooldata['Cohort Graduation Rate Standard Score'], new_schooldata['SchoolWeightedScore'], size=7, kind ='reg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Interestingly, our newly created feature ,'SchoolWeightedScore' depicts a  strong positve correlation to Cohort graduation Standard Score as expected with a pearson R of 0.62. There is also one outlier school which shows up with a moderate SchoolWeightedScore but with a ver low Cohort Graduation Rate Standard school of ~34. When we get to modeling, we will consider removing this outlier school. \n",
    "\n",
    "> Below we explore the benefits of removing that one outlier point in our data to our overrall regression model. Our pearson R improves to 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = new_schooldata[new_schooldata['Cohort Graduation Rate Standard Score']>60]\n",
    "sns.jointplot(temp['Cohort Graduation Rate Standard Score'], temp['SchoolWeightedScore'], size=7, kind ='reg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exceptional Work "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tableau write up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<div class='tableauPlaceholder' id='viz1516834830784' style='position: relative'><noscript><a href='#'><img alt='Dashboard 2 ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;NC&#47;NCDatabyRegion&#47;Dashboard2&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='NCDatabyRegion&#47;Dashboard2' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;NC&#47;NCDatabyRegion&#47;Dashboard2&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1516834830784');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.minWidth='420px';vizElement.style.maxWidth='650px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<div class='tableauPlaceholder' id='viz1517077975911' style='position: relative'><noscript><a href='#'><img alt='Grad Rates ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Gr&#47;GraduationRatesDrilldown&#47;GradRates&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='GraduationRatesDrilldown&#47;GradRates' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Gr&#47;GraduationRatesDrilldown&#47;GradRates&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1517077975911');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='1016px';vizElement.style.height='991px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Drill down bar graph of Graduation Rates by Region, District, and School. (Use tab at top-center to navigate) (Graduation Rate color scale centered at 80%)\n",
    "\n",
    "> The drill down structure of this bar graph allows us to view graduation rates aggregated to the region and district level, as well as per school. We can see at the region level that graduation rates are strong overall with the lowest region being SandHills at 86.07%. As an example of how this visualization can be utilized we will investigate the SandHills reason to locate the problem areas. From the district tab we navigate to the SandHills region column and can see that of it's 13 districts, 2 are below the 80% threshold with another district fairly close to the cut-off. (Charter Schools: 78.60&, Moore County Schools: 77.33%, Montgomery County Schools: 80.40%). To see which schools are bringing these three district's graduation rates down we go to the School tab. When we look at Charter Schools we see that only one school, Flemington Academy (78.60%), of the 9 in this district reported graduation rates. (It is important to note here that there are Charter School districts in all regions so it is necessary to navigate to the SandHill section before looking at the desired Charter School district). In the Moore County Schools district we see that 4 of the 21 schools report graduation rates with the lowest being The Community Learning Center @ Pinckney (38.90%). And finally, in the Montgomery County Schools district 3 of the 10 schools report graduation rates with the lowest being Montgomery Learning Academy (54.20%). Some of these schools do not report graduation rates, as defined by the variable, because they are elementry schools and as such have no bearing on our current analysis. We can use this information on regions, districts, and schools in an effort to quickly locate trouble spots. Once these at-risk schools are found, we can investigate possible causes as to why they have lower than average graduation rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
